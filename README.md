
# ğŸˆ E2E Sport Sales Data Pipeline with Airflow

An end-to-end data pipeline built with ğŸ’ª **Apache Airflow**, designed to simulate real-time sales and customer activity in an American football gear store â€” from raw CSVs to transformed data and production-ready PostgreSQL tables.

> ğŸ¯ **Goal**: Automate the full lifecycle of data â€” ingestion, validation, transformation, and storage â€” using a modern orchestration tool and real cloud-like infrastructure (MinIO, Postgres).

---

## ğŸš€ Features

âœ… Periodic **data simulation** using historical datasets  
âœ… Modular, production-grade **ETL with Airflow DAGs**  
âœ… File-based ingestion via **MinIO** (S3-compatible)  
âœ… Data deduplication, normalization, and transformation  
âœ… Clean database schema with **dimension/fact tables**  
âœ… Scalable structure for **future sales analytics & dashboards**

---

## ğŸ§© Tech Stack

| Layer        | Tool / Technology |
|-------------|-------------------|
| Orchestration | ğŸ›« Apache Airflow |
| Storage      | ğŸª£ MinIO (S3 API) |
| Database     | ğŸ˜ PostgreSQL     |
| Containers   | ğŸ³ Docker / Docker Compose |
| Language     | ğŸ Python         |

---

## ğŸ—ºï¸ Architecture Overview

```
                        +---------------------+
                        |   Historical CSVs   |
                        +----------+----------+
                                   |
                        (Airflow Task: download_data)
                                   |
                        +----------v----------+
                        |     MinIO (S3)      |
                        |  - raw_data bucket  |
                        |  - temp-files       |
                        |  - processed-files  |
                        +----------+----------+
                                   |
                        +----------v----------+
                        | transform_customers |
                        |  - Normalize        |
                        |  - Deduplicate      |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |  upsert_customers   |
                        |   (PostgreSQL DB)   |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |   transform_sales   |
                        |  - Add currency     |
                        |  - Join customers   |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |    upsert_sales     |
                        |   (PostgreSQL DB)   |
                        +---------------------+

```

---

## ğŸ“ Folder Structure

```
.
E2E_SPORT_SALES_PIPELINE_WITH_AIRFLOW/
â”‚
â”œâ”€â”€ .kaggle/                                    # Kaggle API token or dataset references
â”œâ”€â”€ .venv/                                      # Virtual environment directory (excluded from Git)
â”œâ”€â”€ .vscode/                                    # VS Code workspace settings
â”œâ”€â”€ configs/                                    # Configuration files, including constants and credentials
â”‚
â”œâ”€â”€ dags/                                       # Airflow DAGs and associated tasks
â”‚   â”œâ”€â”€ tasks/                                  # Modularized DAG tasks
â”‚   â”‚   â”œâ”€â”€ currencies/                         # Tasks for downloading and processing currency exchange data
â”‚   â”‚   â””â”€â”€ etl/                                # ETL tasks for customers and sales processing
â”‚   â”œâ”€â”€ currency_rates_daily_dag.py             # Airflow DAG to fetch daily currency exchange rates
â”‚   â”œâ”€â”€ extract_daily_to_s3_dag.py              # DAG to simulate daily data arrival into MinIO
â”‚   â””â”€â”€ main_etl_dag.py                         # Master ETL DAG coordinating the end-to-end pipeline
â”‚
â”œâ”€â”€ db/                                         # Database access and DDL scripts
â”‚   â”œâ”€â”€ ddl/                                    # Schema definition scripts
â”‚   â”œâ”€â”€ __init__.py                             # Init file for db module
â”‚   â””â”€â”€ connection.py                           # PostgreSQL connection utility
â”‚
â”œâ”€â”€ envs/                                       # Environment-specific .env files (.env.local, .env.airflow)
â”œâ”€â”€ logs/                                       # Logs generated by Airflow during DAG execution
â”œâ”€â”€ notebooks/                                  # Jupyter notebooks for exploration/debugging
â”œâ”€â”€ pbi_dashboard/                              # Power BI reports and dashboards
â”œâ”€â”€ plugins/                                    # Optional: Custom Airflow plugins
â”œâ”€â”€ raw_data/                                   # Initial raw datasets before ingestion
â”‚
â”œâ”€â”€ scripts/                                    # Local execution scripts
â”‚   â”œâ”€â”€ create_schemas_tables.py                # Creates Postgres schemas and base tables
â”‚   â”œâ”€â”€ download_to_s3_raw.py                   # Downloads raw files and uploads to MinIO
â”‚   â”œâ”€â”€ load_dim_tables.py                      # Loads static dimension tables from CSV
â”‚
â”œâ”€â”€ scripts_drop_truncate_clean/                # Admin scripts for cleaning or truncating tables
â”œâ”€â”€ seeds/                                      # Seed data for dimension tables
â”œâ”€â”€ snowflake_schema/                           # Snowflake schema definitions
â”‚
â”œâ”€â”€ .gitignore                                  # Files/folders excluded from version control
â”œâ”€â”€ complete_file_level_pipeline_analysis.docx  # Documentation describing the full pipeline logic
â”œâ”€â”€ docker-compose.yaml                         # Docker services for Airflow, Postgres, and MinIO
â”œâ”€â”€ main.py                                     # Entrypoint script to run initial setup locally
â”œâ”€â”€ project with airflow and some additions.docx # Additional documentation
â”œâ”€â”€ README.md                                   # ğŸ“˜ Project overview, setup, and usage
â””â”€â”€ requirements.txt                            # Python dependencies

---

## ğŸ› ï¸ Setup & Run

### 1. ğŸ”§ Clone the project

```bash
git clone https://github.com/ATanskiy/e2e_sport_sales_pipeline_with_airflow.git
cd e2e_sport_sales_pipeline_with_airflow
```

### 2. ğŸ³ Start all containers

```bash
docker-compose up -d
```

This brings up:
- PostgreSQL (with 3 schemas: `raw_data`, `prod`, `playground`)
- MinIO with buckets for raw and processed data
- Apache Airflow (webserver, scheduler)

### 3. ğŸŒ Access Airflow UI

Navigate to: [http://localhost:8080](http://localhost:8080)  


## ğŸ” DAG Lifecycle

```
    A[collect_new_files] --> B[transform_customers]
    B --> C[upsert_customers]
    A --> D[transform_sales]
    D --> E[upsert_sales]
    C --> F[move_to_processed]
    E --> F
```

## ğŸ“· Dashboard Previews

Below are preview images of the Power BI dashboard tabs included in this project:

### ğŸ§­ Dashboard Introduction
![Dashboard Introduction](./dashboard/Intruduction.png)  
*A quick overview of the dashboard purpose, data preparation steps, schema design, and navigation tips across tabs.*

### ğŸ“Œ General Overview Tab
![General Overview](./dashboard/1%20General%20overview.png)  
*Shows aggregated sales performance, revenue, costs, customers, and transactions with key slicers for filtering across time, product, store, and channel dimensions.*

### ğŸ“ˆ Overtime Look â€“ Top N Features
![Overtime Look](./dashboard/2%20Overtime%20look%20top%20n%20features.png)  
*Presents line charts of revenue and costs over time, split by payment methods. Useful for trend analysis and comparing top-performing segments.*

### ğŸ—ºï¸ Map â€“ Customers
![Map Customers](./dashboard/3%20Map%20customers.png)  
*Displays geographic distribution of customers across states and cities. Enables identifying regional opportunities or performance gaps.*

### â³ Time Comparison Tab
![Time Comparison](./dashboard/4%20Time%20comparison.png)  
*Enables side-by-side comparison of key metrics (e.g., revenue, costs, transactions) between two custom date ranges. Great for evaluating campaign impact or seasonality.*

---

## ğŸ§  Why This Project Rocks

âœ… Simulates real production workloads  
âœ… Modular, testable Python code  
âœ… Follows data engineering best practices  
âœ… Ready for cloud migration (GCS, AWS, Azure)  
âœ… Clean, reproducible local development via Docker

---

## âœ¨ Author

Created with â¤ï¸ by [@ATanskiy](https://github.com/ATanskiy)

If you find this helpful â€” â­ the repo and share the âš¡ knowledge!

---