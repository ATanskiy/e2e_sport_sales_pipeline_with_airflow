
# 🏈 E2E Sport Sales Data Pipeline with Airflow

An end-to-end data pipeline built with 💪 **Apache Airflow**, designed to simulate real-time sales and customer activity in an American football gear store — from raw CSVs to transformed data and production-ready PostgreSQL tables.

> 🎯 **Goal**: Automate the full lifecycle of data — ingestion, validation, transformation, and storage — using a modern orchestration tool and real cloud-like infrastructure (MinIO, Postgres).

---

## 🚀 Features

✅ Periodic **data simulation** using historical datasets  
✅ Modular, production-grade **ETL with Airflow DAGs**  
✅ File-based ingestion via **MinIO** (S3-compatible)  
✅ Data deduplication, normalization, and transformation  
✅ Clean database schema with **dimension/fact tables**  
✅ Scalable structure for **future sales analytics & dashboards**

---

## 🧩 Tech Stack

| Layer        | Tool / Technology |
|-------------|-------------------|
| Orchestration | 🛫 Apache Airflow |
| Storage      | 🪣 MinIO (S3 API) |
| Database     | 🐘 PostgreSQL     |
| Containers   | 🐳 Docker / Docker Compose |
| Language     | 🐍 Python         |

---

## 🗺️ Architecture Overview

```
                        +---------------------+
                        |   Historical CSVs   |
                        +----------+----------+
                                   |
                        (Airflow Task: download_data)
                                   |
                        +----------v----------+
                        |     MinIO (S3)      |
                        |  - raw_data bucket  |
                        |  - temp-files       |
                        |  - processed-files  |
                        +----------+----------+
                                   |
                        +----------v----------+
                        | transform_customers |
                        |  - Normalize        |
                        |  - Deduplicate      |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |  upsert_customers   |
                        |   (PostgreSQL DB)   |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |   transform_sales   |
                        |  - Add currency     |
                        |  - Join customers   |
                        +----------+----------+
                                   |
                        +----------v----------+
                        |    upsert_sales     |
                        |   (PostgreSQL DB)   |
                        +---------------------+

```

---

## 📁 Folder Structure

```
.
E2E_SPORT_SALES_PIPELINE_WITH_AIRFLOW/
│
├── .kaggle/                                    # Kaggle API token or dataset references
├── .venv/                                      # Virtual environment directory (excluded from Git)
├── .vscode/                                    # VS Code workspace settings
├── configs/                                    # Configuration files, including constants and credentials
│
├── dags/                                       # Airflow DAGs and associated tasks
│   ├── tasks/                                  # Modularized DAG tasks
│   │   ├── currencies/                         # Tasks for downloading and processing currency exchange data
│   │   └── etl/                                # ETL tasks for customers and sales processing
│   ├── currency_rates_daily_dag.py             # Airflow DAG to fetch daily currency exchange rates
│   ├── extract_daily_to_s3_dag.py              # DAG to simulate daily data arrival into MinIO
│   └── main_etl_dag.py                         # Master ETL DAG coordinating the end-to-end pipeline
│
├── db/                                         # Database access and DDL scripts
│   ├── ddl/                                    # Schema definition scripts
│   ├── __init__.py                             # Init file for db module
│   └── connection.py                           # PostgreSQL connection utility
│
├── envs/                                       # Environment-specific .env files (.env.local, .env.airflow)
├── logs/                                       # Logs generated by Airflow during DAG execution
├── notebooks/                                  # Jupyter notebooks for exploration/debugging
├── pbi_dashboard/                              # Power BI reports and dashboards
├── plugins/                                    # Optional: Custom Airflow plugins
├── raw_data/                                   # Initial raw datasets before ingestion
│
├── scripts/                                    # Local execution scripts
│   ├── create_schemas_tables.py                # Creates Postgres schemas and base tables
│   ├── download_to_s3_raw.py                   # Downloads raw files and uploads to MinIO
│   ├── load_dim_tables.py                      # Loads static dimension tables from CSV
│
├── scripts_drop_truncate_clean/                # Admin scripts for cleaning or truncating tables
├── seeds/                                      # Seed data for dimension tables
├── snowflake_schema/                           # Snowflake schema definitions
│
├── .gitignore                                  # Files/folders excluded from version control
├── complete_file_level_pipeline_analysis.docx  # Documentation describing the full pipeline logic
├── docker-compose.yaml                         # Docker services for Airflow, Postgres, and MinIO
├── main.py                                     # Entrypoint script to run initial setup locally
├── project with airflow and some additions.docx # Additional documentation
├── README.md                                   # 📘 Project overview, setup, and usage
└── requirements.txt                            # Python dependencies

---

## 🛠️ Setup & Run

### 1. 🔧 Clone the project

```bash
git clone https://github.com/ATanskiy/e2e_sport_sales_pipeline_with_airflow.git
cd e2e_sport_sales_pipeline_with_airflow
```

### 2. 🐳 Start all containers

```bash
docker-compose up -d
```

This brings up:
- PostgreSQL (with 3 schemas: `raw_data`, `prod`, `playground`)
- MinIO with buckets for raw and processed data
- Apache Airflow (webserver, scheduler)

### 3. 🌍 Access Airflow UI

Navigate to: [http://localhost:8080](http://localhost:8080)  


## 🔁 DAG Lifecycle

```
    A[collect_new_files] --> B[transform_customers]
    B --> C[upsert_customers]
    A --> D[transform_sales]
    D --> E[upsert_sales]
    C --> F[move_to_processed]
    E --> F
```

## 📷 Dashboard Previews

Below are preview images of the Power BI dashboard tabs included in this project:

### 🧭 Dashboard Introduction
![Dashboard Introduction](./dashboard/Intruduction.png)  
*A quick overview of the dashboard purpose, data preparation steps, schema design, and navigation tips across tabs.*

### 📌 General Overview Tab
![General Overview](./dashboard/1%20General%20overview.png)  
*Shows aggregated sales performance, revenue, costs, customers, and transactions with key slicers for filtering across time, product, store, and channel dimensions.*

### 📈 Overtime Look – Top N Features
![Overtime Look](./dashboard/2%20Overtime%20look%20top%20n%20features.png)  
*Presents line charts of revenue and costs over time, split by payment methods. Useful for trend analysis and comparing top-performing segments.*

### 🗺️ Map – Customers
![Map Customers](./dashboard/3%20Map%20customers.png)  
*Displays geographic distribution of customers across states and cities. Enables identifying regional opportunities or performance gaps.*

### ⏳ Time Comparison Tab
![Time Comparison](./dashboard/4%20Time%20comparison.png)  
*Enables side-by-side comparison of key metrics (e.g., revenue, costs, transactions) between two custom date ranges. Great for evaluating campaign impact or seasonality.*

---

## 🧠 Why This Project Rocks

✅ Simulates real production workloads  
✅ Modular, testable Python code  
✅ Follows data engineering best practices  
✅ Ready for cloud migration (GCS, AWS, Azure)  
✅ Clean, reproducible local development via Docker

---

## ✨ Author

Created with ❤️ by [@ATanskiy](https://github.com/ATanskiy)

If you find this helpful — ⭐ the repo and share the ⚡ knowledge!

---