
# ğŸˆ E2E Sport Sales Data Pipeline with Airflow

An end-to-end data pipeline built with ğŸ’ª **Apache Airflow**, designed to simulate real-time sales and customer activity in an American football gear store â€” from raw CSVs to transformed data and production-ready PostgreSQL tables.

> ğŸ¯ **Goal**: Automate the full lifecycle of data â€” ingestion, validation, transformation, and storage â€” using a modern orchestration tool and real cloud-like infrastructure (MinIO, Postgres).

---

## ğŸš€ Features

âœ… Periodic **data simulation** using historical datasets  
âœ… Modular, production-grade **ETL with Airflow DAGs**  
âœ… File-based ingestion via **MinIO** (S3-compatible)  
âœ… Data deduplication, normalization, and transformation  
âœ… Clean database schema with **dimension/fact tables**  
âœ… Scalable structure for **future sales analytics & dashboards**

---

## ğŸ§© Tech Stack

| Layer        | Tool / Technology |
|-------------|-------------------|
| Orchestration | ğŸ›« Apache Airflow |
| Storage      | ğŸª£ MinIO (S3 API) |
| Database     | ğŸ˜ PostgreSQL     |
| Containers   | ğŸ³ Docker / Docker Compose |
| Language     | ğŸ Python         |

---

## ğŸ—ºï¸ Architecture Overview

```text
                        +---------------------+
                        |   Historical CSVs   |
                        +----------+----------+
                                   |
                        (Airflow Task: download_data)
                                   |
                        +----------v----------+
                        |     MinIO (S3)      |
                        |  - raw_data bucket  |
                        |  - temp-files       |
                        |  - processed-files  |
                        +----------+----------+
                                   |
             +---------------------+---------------------+
             |                                           |
+------------v-----------+                +--------------v-----------+
|  transform_customers() |                |   transform_sales()      |
|  - Clean/normalize     |                |  - Add currency rates    |
|  - Deduplicate records |                |  - Join customers        |
+------------+-----------+                +--------------+-----------+
             |                                           |
   +---------v----------+                      +---------v----------+
   | upsert_customers() |                      |  upsert_sales()    |
   |  (PostgreSQL DB)   |                      |  (PostgreSQL DB)   |
   +--------------------+                      +--------------------+
```

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ main_etl_dag.py        # Main DAG definition
â”œâ”€â”€ scripts/                   # Bootstrap & infra helpers
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ customers/             # Customer ETL logic
â”‚   â””â”€â”€ sales/                 # Sales ETL logic
â”œâ”€â”€ configs/                   # Env vars, S3 config, schemas
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Setup & Run

### 1. ğŸ”§ Clone the project

```bash
git clone https://github.com/ATanskiy/e2e_sport_sales_pipeline_with_airflow.git
cd e2e_sport_sales_pipeline_with_airflow
```

### 2. ğŸ³ Start all containers

```bash
docker-compose up -d
```

This brings up:
- PostgreSQL (with 3 schemas: `raw_data`, `prod`, `playground`)
- MinIO with buckets for raw and processed data
- Apache Airflow (webserver, scheduler)

### 3. ğŸŒ Access Airflow UI

Navigate to: [http://localhost:8080](http://localhost:8080)  


## ğŸ” DAG Lifecycle

```
    A[collect_new_files] --> B[transform_customers]
    B --> C[upsert_customers]
    A --> D[transform_sales]
    D --> E[upsert_sales]
    C --> F[move_to_processed]
    E --> F
```

## ğŸ§  Why This Project Rocks

âœ… Simulates real production workloads  
âœ… Modular, testable Python code  
âœ… Follows data engineering best practices  
âœ… Ready for cloud migration (GCS, AWS, Azure)  
âœ… Clean, reproducible local development via Docker

---

## âœ¨ Author

Created with â¤ï¸ by [@ATanskiy](https://github.com/ATanskiy)

If you find this helpful â€” â­ the repo and share the âš¡ knowledge!

---